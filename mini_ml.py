# -*- coding: utf-8 -*-
"""mini ml

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UDzpXWIRDbFdi6gQiYRjG_JAJU0vnrMo
"""

import pandas as pd
df = pd.read_csv('/content/mimi_ml (1).zip')   # adjust filename if different
df.head()

# Step 1 â€” load (run now)
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer(as_frame=True)
X = data.frame.drop(columns=['target'])
y = data.frame['target']
print("X shape:", X.shape, "y counts:\n", y.value_counts())

# Step 2 â€” small EDA
import matplotlib.pyplot as plt
import seaborn as sns
print(X.describe().T.head())
sns.countplot(x=y.map({0:'malignant',1:'benign'}))
plt.title('Class distribution'); plt.show()
# show first 5 features distributions
X.iloc[:, :5].hist(bins=20, figsize=(10,4)); plt.show()

# Step 3 â€” split + scaling
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
scaler = StandardScaler().fit(X_train)            # fit only on train
X_train_s = scaler.transform(X_train)
X_test_s  = scaler.transform(X_test)
print("Train/test shapes:", X_train.shape, X_test.shape)

# Step 4 â€” baseline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train_s, y_train)
y_pred = lr.predict(X_test_s)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, digits=4))

# Step 5 â€” Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)   # RF works fine without scaling
y_pred_rf = rf.predict(X_test)
from sklearn.metrics import classification_report, roc_auc_score
print(classification_report(y_test, y_pred_rf, digits=4))
y_prob_rf = rf.predict_proba(X_test)[:,1]
print("Test ROC AUC:", roc_auc_score(y_test, y_prob_rf))

# Step 6 â€” cross-validation (AUC)
from sklearn.model_selection import StratifiedKFold, cross_val_score
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
import numpy as np
lr_auc = cross_val_score(lr, X, y, cv=cv, scoring='roc_auc')
rf_auc = cross_val_score(RandomForestClassifier(n_estimators=200, random_state=42), X, y, cv=cv, scoring='roc_auc')
print("LR AUC mean/std:", np.mean(lr_auc).round(4), np.std(lr_auc).round(4))
print("RF  AUC mean/std:", np.mean(rf_auc).round(4), np.std(rf_auc).round(4))

# Step 7 â€” evaluation plots & top features (RF)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

# confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt='d', xticklabels=data.target_names, yticklabels=data.target_names)
plt.title('RF Confusion Matrix'); plt.show()

# ROC
fpr, tpr, _ = roc_curve(y_test, y_prob_rf)
plt.plot(fpr, tpr, label=f'RF AUC={roc_auc_score(y_test,y_prob_rf):.4f}')
plt.plot([0,1],[0,1],'k--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.show()

# feature importances
import pandas as pd
feat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False).head(15)
feat_imp.plot.bar(title='Top 15 RF feature importances'); plt.show()
print(feat_imp.round(4))

# Step 8 â€” save model and scaler
import joblib
joblib.dump(rf, 'model_rf.joblib')
joblib.dump(scaler, 'scaler.joblib')
print("Saved model_rf.joblib and scaler.joblib")

from sklearn.linear_model import LogisticRegression

# Step 9: Train the model
model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

print("âœ… Model training completed!")

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Step 10: Predict on test data
y_pred = model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("ðŸŽ¯ Accuracy:", accuracy)
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Create DataFrame instead of array
sample_data = pd.DataFrame([X_test.iloc[0]], columns=X_test.columns)
prediction = model.predict(sample_data)

print("ðŸ§© Actual Label:", y_test.iloc[0])
print("ðŸ¤– Predicted Label:", prediction[0])

# Step 12: Visualize model performance

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Visualize with seaborn heatmap
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Breast Cancer Prediction')
plt.xlabel('Predicted Labels')
plt.ylabel('Actual Labels')
plt.show()

# Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… Model Accuracy: {accuracy * 100:.2f}%")

from sklearn.preprocessing import StandardScaler

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Logistic Regression model
log_reg = LogisticRegression()

# Define hyperparameter grid
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs'],
    'max_iter': [100, 200, 300]
}

# Grid search
grid_lr = GridSearchCV(log_reg, param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1)
grid_lr.fit(X_train_scaled, y_train)

print("âœ… Best Parameters for Logistic Regression:", grid_lr.best_params_)
print("ðŸ“Š Best Accuracy Score:", grid_lr.best_score_)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)

# Smaller parameter grid (to speed up)
param_grid_rf = {
    'n_estimators': [50, 100],         # fewer trees
    'max_depth': [5, 10, None],        # shallower trees
    'min_samples_split': [2, 5],       # fewer splits to test
}

# Reduce CV folds to 3 (instead of 5)
grid_rf = GridSearchCV(rf, param_grid_rf, cv=3, scoring='accuracy', n_jobs=-1)

# Fit the model (should finish in <1 minute)
grid_rf.fit(X_train, y_train)

print("âœ… Best Parameters for Random Forest:", grid_rf.best_params_)
print("ðŸ“Š Best Accuracy Score:", grid_rf.best_score_)

best_rf = grid_rf.best_estimator_   # Best tuned model
y_pred_tuned = best_rf.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report

print("ðŸ† Tuned Random Forest Test Accuracy:", accuracy_score(y_test, y_pred_tuned) * 100)
print("\nðŸ“Š Classification Report:\n", classification_report(y_test, y_pred_tuned))

# Example input (30 features)
example_input = [[
    17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871,
    1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193,
    25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189
]]

# Make prediction
prediction = best_rf.predict(example_input)

# Show result
if prediction[0] == 0:
    print("ðŸ”µ Prediction: Benign (Non-cancerous)")
else:
    print("ðŸ”´ Prediction: Malignant (Cancerous)")